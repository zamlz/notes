:PROPERTIES:
:ID:       3d770134-2e93-4397-a846-227e71f2fe31
:ROAM_ALIASES: Agent
:END:
#+TITLE: RL Agent
#+AUTHOR: Amlesh Sivanantham (zamlz)
#+CREATED: [2021-04-03 Sat 20:48]
#+LAST_MODIFIED: [2021-07-19 Mon 09:43:25]
#+DOWNLOADED: screenshot @ 2021-04-03 20:00:42
#+filetags: ARTIFICIAL_INTELLIGENCE RL

[[file:data/rl_agent_brain.png]]

At every step, our agent sees the [[id:9e3eca4d-4109-46f9-8632-45c02dcead4e][RL Environment]] and [[id:ed78ebfa-633f-44f3-bb4f-d6c946df8386][Reward Signal]] and must make an action. /The agent can only make actions which the [[id:9e3eca4d-4109-46f9-8632-45c02dcead4e][RL Environment]] exposes./

The agent at each timestep \(t\) :
- performance action \(a_t\)
- recieves new observation \(o_t\)
- recieves new reward \(r_t\)

* Agent History

As the agent will get those values at every timestep, we can define the history to be a sequence of observations, actions and rewards that the agent has seen so far.

\[H_t = a_1, o_1, r_1, \dots, a_t, o_t, r_t\]

(All observations up to time \(t\))

* Agent State

The Agent will take a look at this history in order to make a decision. However the history is really not that useful. There can be far too much data. Instead it would be better if we could create a *State* that encapsulates the history of the system up to that point. Formally,

\[S_t = f(H_t)\]

The function \(f\) here is simply a mapping from \(H_t\) to \(S_t\). It is up to us to design this mapping.

The agent can learn its own state \(S^a_t\) built from what its seen. We build this from \(H_t\) since this is all data that is given to the agent. We will use this state to make deicions.

* Major Components of the RL Agent

The agent typicallyl includes the following pieces:
- [[id:f7baf9ab-82a4-4ce4-a2dd-c5a07c3e05c2][Policy Function]]
- [[id:3d1b1c4e-cc16-410a-b385-a29244afb66a][Value Function]]
- [[id:dce31907-d18c-4d81-a043-9cb889220cc8][Environment Model]]

The agent doesn't have to make use of all of these components, but will typically use some of them. We can taxonomize different Agents based on which components they have. They can have the following types:
- [[id:496fdd9a-771e-4599-aac1-f17736405a7b][Value Based Agent]]
- [[id:a4f46bf0-3bf9-4c0e-9272-82d1cbada07c][Policy Based Agent]]
- [[id:49eb8c6b-e2fa-48c0-ac12-05b7a743bce3][Actor-Critic Agent]]

Each of these agents can now have an internal [[id:dce31907-d18c-4d81-a043-9cb889220cc8][Environment Model]].
- [[id:2b44d7df-a9ac-48e4-a2f0-eb4255115775][Model-Free RL]]
- [[id:b2b21972-fadc-4ff6-9a6a-2263c2dd19bf][Model-Based RL]]

#+DOWNLOADED: screenshot @ 2021-04-03 22:01:16
[[file:data/rl_agent_types.png]]
