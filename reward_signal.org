:PROPERTIES:
:ID:       ed78ebfa-633f-44f3-bb4f-d6c946df8386
:ROAM_ALIASES: Reward
:END:
#+TITLE: Reward Signal
#+AUTHOR: Amlesh Sivanantham (zamlz)
#+CREATED: [2021-04-03 Sat 20:45]
#+LAST_MODIFIED: [2021-07-19 Mon 09:43:25]
#+filetags: ARTIFICIAL_INTELLIGENCE NEUROSCIENCE RL

The goal of the [[id:3d770134-2e93-4397-a846-227e71f2fe31][RL Agent]] is to get as much reward as possible. At every timestep, the [[id:3d770134-2e93-4397-a846-227e71f2fe31][RL Agent]] gets \(R_t\) as a scalar feedback signal from the [[id:9e3eca4d-4109-46f9-8632-45c02dcead4e][Environment]] and so we wish to maximize the cumulative sum of this signal.

This is the reward hypothesis:

#+begin_quote
All goals can be described by the maximization of expected cumulative reward
#+end_quote

/"Is this statement really valid though?"/ David Silver asks...

We can define the reward to be given at any point. We can give the +ve/-ve reward after every action or after a sequence of actions. In even more extreme cases, we can give reward at the end of the game (if we win or loose in chess for example). Another strategy is simply giving reward equivalent to the change in score of a game. Defining a reward function actually turns to be a challenging task in its own right for RL.

Because of the nature of rewards, they can be delayed. We cannot be greedy and look for short term rewards. It might be better to forgo short term rewards and instead get a much larger reward in the future.
