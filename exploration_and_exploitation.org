:PROPERTIES:
:ID:       1340c8dd-3793-4f62-a73f-a4f0160389ed
:END:
#+TITLE: Exploration and Exploitation
#+AUTHOR: Amlesh Sivanantham (zamlz)
#+CREATED: [2021-04-03 Sat 22:09]
#+LAST_MODIFIED: [2021-07-19 Mon 09:43:25]
#+filetags: RL ARTIFICIAL_INTELLIGENCE

A fundamental problem in [[id:3d219429-3c1a-4c81-b135-b2723b0f5d0e][Reinforcement Learning]] is whether to leverage exploration in order to find new strategies or states with better [[id:ed78ebfa-633f-44f3-bb4f-d6c946df8386][Reward]], or to exploit existing knowledge of strategies and states to get existing [[id:ed78ebfa-633f-44f3-bb4f-d6c946df8386][Reward Signal]]. How do we leverage new and existing experiences to safely explore the environment in order to gain new (better) reward?
